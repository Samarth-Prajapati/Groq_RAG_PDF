# ğŸ“„ PDF-based Q&A RAG Application

A **Retrieval-Augmented Generation (RAG)** based Questionâ€“Answering application that allows users to upload a PDF and ask questions strictly grounded in the document content. The app is built using **Streamlit**, **LangChain**, **FAISS**, **HuggingFace embeddings**, and **Groq-hosted LLMs**.

---

## Streamlit Deployment
**LINK** : https://rag-pdf-by-sam.streamlit.app/

---

## ğŸš€ Features

* Upload any **PDF document**
* Automatic **text extraction and chunking**
* **Vector database creation** using FAISS
* Semantic search using **HuggingFace sentence embeddings**
* Context-aware answers generated by **Groq LLaMA model**
* Prevents hallucination by answering **only from document context**
* Simple and interactive **Streamlit UI**

---

## ğŸ§  Architecture (RAG Pipeline)

```
PDF Upload
   â†“
Temporary File Storage
   â†“
PyPDFLoader (Text Extraction)
   â†“
Text Chunking
   â†“
HuggingFace Embeddings
   â†“
FAISS Vector Store
   â†“
Retriever
   â†“
Prompt + Groq LLM
   â†“
Answer
```

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **Streamlit** â€“ Web UI
* **LangChain** â€“ RAG orchestration
* **Groq (LLaMA 3.1)** â€“ Large Language Model
* **HuggingFace Sentence Transformers** â€“ Embeddings
* **FAISS** â€“ Vector similarity search
* **python-dotenv** â€“ Environment variable management

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ .env                # Environment variables (not committed)
â”œâ”€â”€ requirements.txt    # Project dependencies
â”œâ”€â”€ README.md           # Project documentation
```

---

## ğŸ” Environment Setup

Create a `.env` file in the root directory:

```
GROQ_API_KEY=your_groq_api_key_here
```

---

## ğŸ“¦ Installation (Using Conda â€“ Recommended)

```bash
conda create -n genai python=3.11 -y
conda activate genai
pip install -r requirements.txt
```

---

## â–¶ï¸ Running the Application

```bash
streamlit run app.py
```

Then open your browser at:

```
http://localhost:8501
```

---

## âš™ï¸ How It Works

1. User uploads a PDF file
2. PDF is temporarily saved on the system
3. Text is extracted using `PyPDFLoader`
4. Text is split into overlapping chunks
5. Embeddings are generated using HuggingFace model
6. FAISS stores vectors for fast retrieval
7. User question retrieves relevant chunks
8. Groq-hosted LLaMA model generates answer

The model is **strictly instructed not to hallucinate** if the answer is not present in the document.

---

## ğŸ§ª Example Use Cases

* Academic document Q&A
* Research paper analysis
* Company policy assistant
* Legal or technical document search
* Interview or study preparation

---

## ğŸ“Š Performance Notes

* Initial PDF processing may take time depending on size
* Smaller PDFs respond faster
* Embeddings are created only once per session

---

## ğŸ”® Future Enhancements

* Multi-PDF upload support
* Source citations in answers
* Persistent vector database
* Streaming responses
* Authentication and user sessions

---

## ğŸ‘¨â€ğŸ’» Author

**Samarth Prajapati**
AI / ML & Generative AI Learner

---

## â­ Acknowledgements

* Krish Naik â€“ Generative AI guidance
* LangChain community
* Groq platform
* Streamlit team

---

â­ If you find this project useful, feel free to star and extend it!
